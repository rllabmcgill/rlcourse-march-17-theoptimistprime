{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-policy Control with Approximation\n",
    "\n",
    "\n",
    "\n",
    "Tabular approach to approximate value functions can't handle RL problems with large/continuous state-space. To achieve this, any of a broad range of existing methods for supervised-learning function approximation can be used simply by treating each backup as a training example. Parameterized function approximation can very easily be used here. \n",
    "\n",
    "The approximate value function is represented as a parameterized functional form with weight vector $\\theta \\in \\mathbb{R}^n$. Although the weight vector has many components (n), $|\\mathbb{S}| >> n$ and we must settle for an approximate solution. The parametric approximation of the action-value function $\\hat{q}(s,a,\\theta) \\approx q^{*}(s,a)$. We can use MSE($\\theta$) as a measure of the error in the value function. The optimal value function can be achieved using stochastic gradient descent (SGD) on weight vector.\n",
    "\n",
    "General SGD is : $$ \\theta_{t+1} \\leftarrow \\theta_{t} - \\alpha \\nabla_{\\theta}(Error_t)^2 $$\n",
    "where Error in case of state-action value function can be represented as -\n",
    "$$Error_t = Target_t - \\hat{q}(s,a,\\theta)$$\n",
    "\n",
    "\n",
    "## Semi-gradient Methods\n",
    "\n",
    "Target value can be computed using a lot of different ways like MC, TD(0), n-Step return etc. Unless target value is computed using MC methods, bootstrapping from current estimate of weight vector $\\theta$ is needed to approximate future rewards. Therefore, for all methods except MC, target value is a function of weight vector. True gradient of Error is complex to evaluate and doesn't offer much benefit over the alternative - Semi-gradient methods. \n",
    "\n",
    "In Semi-gradient TD methods, the weight vector appears in the update target, yet this is not taken into account in computing the gradient thus the name. As such, they cannot rely on classical SGD results. Nevertheless, good results can be obtained for semi-gradient methods in the special case of linear function approximation, in which the value estimates are $\\phi.\\theta$ .\n",
    "\n",
    "## Feature Selection\n",
    "Choosing the features is one of the most important ways of adding prior domain knowledge to reinforcement learning systems. Features can be chosen as polynomials, but this case generalizes poorly in the online setting. Features can also be based on Fourier basis functions, Radial basis functions or some form of coarse coding. Tile coding is a form of coarse coding that is particularly computationally efficient and flexible.\n",
    "\n",
    "## Tile Coding\n",
    "In tile coding the receptive fields of the features are grouped into partitions of the input space. Each such partition is called a tiling, and each element of the partition is called a tile. Feature of a state in case of tile coding is the tile in which the state falls; generalization would be have overlapping receptive field. To get true coarse coding with tile coding, multiple tilings are needed, each offset by a fraction of a tile width. Every state falls in exactly one tile in each of the 'n' tilings. These 'n' tiles correspond to n features that become active when the state occurs. Therefore, the feature vector $\\phi(s)$ has one component for each tile in\n",
    "each tiling. \n",
    "\n",
    "\n",
    "## Experiment\n",
    "\n",
    "Here, I experiment with Mountain Car domain. Aim is to get an under powered car to the top of a hill (top = 0.5 position). The car is on a one-dimensional track, positioned between two \"mountains\". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "Observation for this domain is car's position and it's velocity. Car's position can be between -1.2 and 0.5 with max velocity 0.07 in either direction. A reward of -1 is received for everytime step. Episode starts with a random position from -0.6 to -0.4 and no velocity.\n",
    "\n",
    "As the state space is very large, I'll be using a linear approximation for q as $\\phi.\\theta$ where $\\phi$ represents features corresponding to <state,action>pair. Features are obtained from tilecoding. As we approach optimal state-action value function $q^*$, time spend in every episode should decrease. I'll use this to test my agent's ability to learn a good approximation of q. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from TileCoding import *\n",
    "from utils import *\n",
    "\n",
    "EPSILON = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mountain Car Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MountainCar(object):\n",
    "    class observations(object):\n",
    "        def __init__(self):\n",
    "            self.POS_MIN = -1.2\n",
    "            self.POS_MAX = 0.5\n",
    "            self.VEL_MIN = -0.07\n",
    "            self.VEL_MAX = 0.07\n",
    "            self.high = np.array([self.POS_MAX, self.VEL_MAX])\n",
    "            self.low = np.array([self.POS_MIN, self.VEL_MIN])\n",
    "\n",
    "        def reset(self):\n",
    "            self.pos = np.random.uniform(-0.6, -0.4)\n",
    "            self.vel = 0.\n",
    "            return (self.pos, self.vel)\n",
    "\n",
    "        def act(self, action):\n",
    "            newVel = self.vel + 0.001 * action - 0.0025 * np.cos(3 * self.pos)\n",
    "            newVel = min(max(self.VEL_MIN, newVel), self.VEL_MAX)\n",
    "            newPos = self.pos + newVel\n",
    "            newPos = min(max(self.POS_MIN, newPos), self.POS_MAX)\n",
    "            if newPos == self.POS_MIN:\n",
    "                newVel = 0.0\n",
    "            self.pos = newPos\n",
    "            self.vel = newVel\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ACT_LEFT = -1\n",
    "        self.ACT_NONE = 0\n",
    "        self.ACT_RIGHT = 1\n",
    "        self.actions = [self.ACT_LEFT, self.ACT_NONE, self.ACT_RIGHT]\n",
    "        self.observation_space = self.observations()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.record =[]\n",
    "        return self.observation_space.reset()\n",
    "\n",
    "    def goalReached(self):\n",
    "        return (self.observation_space.pos == self.observation_space.POS_MAX)\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = -1\n",
    "        self.record.append(([self.observation_space.pos, self.observation_space.vel], action, reward))\n",
    "        self.observation_space.act(action)\n",
    "        return ([self.observation_space.pos, self.observation_space.vel],reward, self.goalReached())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent based on Semi-gradient SARSA with tile coding for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TileCodingAgent(object):\n",
    "    def __init__(self, env, stepSize, numOfTilings=8, maxSize=2048, n=1):\n",
    "        self.env = env\n",
    "        self.maxSize = maxSize\n",
    "        self.nStep = n\n",
    "        self.numOfTilings = numOfTilings\n",
    "        self.stepSize = stepSize / numOfTilings\n",
    "        self.hashTable = IHT(maxSize)\n",
    "        self.weights = np.zeros(maxSize)\n",
    "        # observations need to be scaled\n",
    "        self.Scale = self.numOfTilings / (env.observation_space.high - env.observation_space.low)\n",
    "\n",
    "    def getTiles(self, obs, action):\n",
    "        activeTiles = tiles(self.hashTable, self.numOfTilings, obs*self.Scale, [action])\n",
    "        return activeTiles\n",
    "\n",
    "    def value(self, obs, action):\n",
    "        if obs[0] == self.env.observation_space.POS_MAX:\n",
    "            return 0.0\n",
    "        f = self.getTiles(obs, action)\n",
    "        return np.sum(self.weights[f])\n",
    "\n",
    "    def learn(self, obs, action, target):\n",
    "        features = self.getTiles(obs, action)\n",
    "        estimation = np.sum(self.weights[features])\n",
    "        delta = self.stepSize * (target - estimation)\n",
    "        for f in features:\n",
    "            self.weights[f] += delta\n",
    "\n",
    "    def getAction(self, obs):\n",
    "        if np.random.binomial(1, EPSILON) == 1:\n",
    "            return np.random.choice(self.env.actions)\n",
    "        values = []\n",
    "        for action in self.env.actions:\n",
    "            values.append(self.value(obs, action))\n",
    "        return argmax(values) - 1\n",
    "\n",
    "    def generateEpisode(self):\n",
    "        obs = self.env.reset()\n",
    "        action = self.getAction(obs)\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs, _, done = self.env.step(action)\n",
    "            action = self.getAction(obs)\n",
    "\n",
    "    def semiGradientNStepSarsa(self):\n",
    "        episode = self.env.record\n",
    "        T = len(episode)\n",
    "        for t in range(T):\n",
    "            ret = 0.\n",
    "            for i in range(t,min(T,t+self.nStep)):\n",
    "                ret += episode[i][2]\n",
    "            if t+self.nStep<T:\n",
    "                ret += self.value(episode[t+self.nStep][0], episode[t+self.nStep][1])\n",
    "            self.learn(episode[t][0], episode[t][1], ret)\n",
    "        return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence with n-Step returns for targets. I tested with value of 1-step and 4-step returns. As expected, agent with 4-step return converged faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "runs = 5\n",
    "episodes = 500\n",
    "numOfTilings = 8\n",
    "alpha = 0.1\n",
    "nSteps = [1,4]\n",
    "\n",
    "env = MountainCar()\n",
    "timesteps = np.zeros((len(nSteps), episodes))\n",
    "for run in range(runs):\n",
    "    for i in range(len(nSteps)):\n",
    "        agent = TileCodingAgent(env, 0.1, n=nSteps[i])\n",
    "        for episode in range(0, episodes):\n",
    "            agent.generateEpisode()\n",
    "            timesteps[i, episode] += agent.semiGradientNStepSarsa()\n",
    "\n",
    "timesteps /= runs\n",
    "\n",
    "for i in range(0, len(nSteps)):\n",
    "    plt.plot(timesteps[i], label='n-Step Return = '+str(nSteps[i]))\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Time Steps per episode')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](n_step_reward_comparision.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
